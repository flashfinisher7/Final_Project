{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0df0a10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries for loading and viewing of data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60a597e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the data which is present in 5 folders\n",
    "Folder0 = pd.read_csv(r\"C:\\Users\\mvsan\\Desktop\\New folder (2)\\AdienceBenchmarkGenderAndAgeClassification\\fold_0_data.txt\",sep = \"\\t\")\n",
    "Folder1 = pd.read_csv(r\"C:\\Users\\mvsan\\Desktop\\New folder (2)\\AdienceBenchmarkGenderAndAgeClassification\\fold_1_data.txt\",sep = \"\\t\")\n",
    "Folder2 = pd.read_csv(r\"C:\\Users\\mvsan\\Desktop\\New folder (2)\\AdienceBenchmarkGenderAndAgeClassification\\fold_2_data.txt\",sep = \"\\t\")\n",
    "Folder3 = pd.read_csv(r\"C:\\Users\\mvsan\\Desktop\\New folder (2)\\AdienceBenchmarkGenderAndAgeClassification\\fold_3_data.txt\",sep = \"\\t\")\n",
    "Folder4 = pd.read_csv(r\"C:\\Users\\mvsan\\Desktop\\New folder (2)\\AdienceBenchmarkGenderAndAgeClassification\\fold_4_data.txt\",sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b1ca49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The five folder data is added into single folder that Data\n",
    "Data=pd.concat([Folder0,Folder1,Folder2,Folder3,Folder4],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ec4b23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19370 entries, 0 to 19369\n",
      "Data columns (total 12 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   user_id             19370 non-null  object\n",
      " 1   original_image      19370 non-null  object\n",
      " 2   face_id             19370 non-null  int64 \n",
      " 3   age                 19370 non-null  object\n",
      " 4   gender              18591 non-null  object\n",
      " 5   x                   19370 non-null  int64 \n",
      " 6   y                   19370 non-null  int64 \n",
      " 7   dx                  19370 non-null  int64 \n",
      " 8   dy                  19370 non-null  int64 \n",
      " 9   tilt_ang            19370 non-null  int64 \n",
      " 10  fiducial_yaw_angle  19370 non-null  int64 \n",
      " 11  fiducial_score      19370 non-null  int64 \n",
      "dtypes: int64(8), object(4)\n",
      "memory usage: 1.8+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>original_image</th>\n",
       "      <th>face_id</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>dx</th>\n",
       "      <th>dy</th>\n",
       "      <th>tilt_ang</th>\n",
       "      <th>fiducial_yaw_angle</th>\n",
       "      <th>fiducial_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30601258@N03</td>\n",
       "      <td>10399646885_67c7d20df9_o.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>(25, 32)</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>414</td>\n",
       "      <td>1086</td>\n",
       "      <td>1383</td>\n",
       "      <td>-115</td>\n",
       "      <td>30</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30601258@N03</td>\n",
       "      <td>10424815813_e94629b1ec_o.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>(25, 32)</td>\n",
       "      <td>m</td>\n",
       "      <td>301</td>\n",
       "      <td>105</td>\n",
       "      <td>640</td>\n",
       "      <td>641</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30601258@N03</td>\n",
       "      <td>10437979845_5985be4b26_o.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>(25, 32)</td>\n",
       "      <td>f</td>\n",
       "      <td>2395</td>\n",
       "      <td>876</td>\n",
       "      <td>771</td>\n",
       "      <td>771</td>\n",
       "      <td>175</td>\n",
       "      <td>-30</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30601258@N03</td>\n",
       "      <td>10437979845_5985be4b26_o.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>(25, 32)</td>\n",
       "      <td>m</td>\n",
       "      <td>752</td>\n",
       "      <td>1255</td>\n",
       "      <td>484</td>\n",
       "      <td>485</td>\n",
       "      <td>180</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30601258@N03</td>\n",
       "      <td>11816644924_075c3d8d59_o.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>(25, 32)</td>\n",
       "      <td>m</td>\n",
       "      <td>175</td>\n",
       "      <td>80</td>\n",
       "      <td>769</td>\n",
       "      <td>768</td>\n",
       "      <td>-75</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id                original_image  face_id       age gender     x  \\\n",
       "0  30601258@N03  10399646885_67c7d20df9_o.jpg        1  (25, 32)      f     0   \n",
       "1  30601258@N03  10424815813_e94629b1ec_o.jpg        2  (25, 32)      m   301   \n",
       "2  30601258@N03  10437979845_5985be4b26_o.jpg        1  (25, 32)      f  2395   \n",
       "3  30601258@N03  10437979845_5985be4b26_o.jpg        3  (25, 32)      m   752   \n",
       "4  30601258@N03  11816644924_075c3d8d59_o.jpg        2  (25, 32)      m   175   \n",
       "\n",
       "      y    dx    dy  tilt_ang  fiducial_yaw_angle  fiducial_score  \n",
       "0   414  1086  1383      -115                  30              17  \n",
       "1   105   640   641         0                   0              94  \n",
       "2   876   771   771       175                 -30              74  \n",
       "3  1255   484   485       180                   0              47  \n",
       "4    80   769   768       -75                   0              34  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking data info and first & last five records from dataframe.  \n",
    "Data.info()\n",
    "Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef2260cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>original_image</th>\n",
       "      <th>face_id</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>dx</th>\n",
       "      <th>dy</th>\n",
       "      <th>tilt_ang</th>\n",
       "      <th>fiducial_yaw_angle</th>\n",
       "      <th>fiducial_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19365</th>\n",
       "      <td>7153718@N04</td>\n",
       "      <td>11598838386_349a0d4849_o.jpg</td>\n",
       "      <td>2282</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>596</td>\n",
       "      <td>460</td>\n",
       "      <td>1472</td>\n",
       "      <td>1473</td>\n",
       "      <td>-75</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19366</th>\n",
       "      <td>7153718@N04</td>\n",
       "      <td>11598166203_c70bb34c80_o.jpg</td>\n",
       "      <td>2283</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1179</td>\n",
       "      <td>755</td>\n",
       "      <td>331</td>\n",
       "      <td>331</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19367</th>\n",
       "      <td>7153718@N04</td>\n",
       "      <td>11598166203_c70bb34c80_o.jpg</td>\n",
       "      <td>2282</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1201</td>\n",
       "      <td>1179</td>\n",
       "      <td>293</td>\n",
       "      <td>293</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19368</th>\n",
       "      <td>7153718@N04</td>\n",
       "      <td>11598145163_733cb99713_o.jpg</td>\n",
       "      <td>2282</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1027</td>\n",
       "      <td>946</td>\n",
       "      <td>408</td>\n",
       "      <td>408</td>\n",
       "      <td>-85</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19369</th>\n",
       "      <td>7153718@N04</td>\n",
       "      <td>11598013005_240c2bc9c7_o.jpg</td>\n",
       "      <td>2282</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>498</td>\n",
       "      <td>643</td>\n",
       "      <td>772</td>\n",
       "      <td>772</td>\n",
       "      <td>-80</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           user_id                original_image  face_id   age gender     x  \\\n",
       "19365  7153718@N04  11598838386_349a0d4849_o.jpg     2282  None    NaN   596   \n",
       "19366  7153718@N04  11598166203_c70bb34c80_o.jpg     2283  None    NaN  1179   \n",
       "19367  7153718@N04  11598166203_c70bb34c80_o.jpg     2282  None    NaN  1201   \n",
       "19368  7153718@N04  11598145163_733cb99713_o.jpg     2282  None    NaN  1027   \n",
       "19369  7153718@N04  11598013005_240c2bc9c7_o.jpg     2282  None    NaN   498   \n",
       "\n",
       "          y    dx    dy  tilt_ang  fiducial_yaw_angle  fiducial_score  \n",
       "19365   460  1472  1473       -75                   0              30  \n",
       "19366   755   331   331         5                   0             108  \n",
       "19367  1179   293   293         5                   0              99  \n",
       "19368   946   408   408       -85                   0              49  \n",
       "19369   643   772   772       -80                   0             111  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cheacking the data last five records of dataframe\n",
    "Data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7411e24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19370, 12)\n"
     ]
    }
   ],
   "source": [
    "#Checking Data shape\n",
    "print(Data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca249295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#ploting a graph for seeing gender in taken data\n",
    "gender = ['f','m','u']\n",
    "plt.bar(gender,Data.gender.value_counts(), align='center', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b4dee94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we start import necessary module for building model\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Activation, Dropout, Flatten, Dense, Dropout, LayerNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b044b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19370 entries, 0 to 19369\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   age     19370 non-null  object\n",
      " 1   gender  18591 non-null  object\n",
      " 2   x       19370 non-null  int64 \n",
      " 3   y       19370 non-null  int64 \n",
      " 4   dx      19370 non-null  int64 \n",
      " 5   dy      19370 non-null  int64 \n",
      "dtypes: int64(4), object(2)\n",
      "memory usage: 908.1+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>dx</th>\n",
       "      <th>dy</th>\n",
       "      <th>img_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(25, 32)</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>414</td>\n",
       "      <td>1086</td>\n",
       "      <td>1383</td>\n",
       "      <td>AdienceBenchmarkGenderAndAgeClassification/fac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(25, 32)</td>\n",
       "      <td>m</td>\n",
       "      <td>301</td>\n",
       "      <td>105</td>\n",
       "      <td>640</td>\n",
       "      <td>641</td>\n",
       "      <td>AdienceBenchmarkGenderAndAgeClassification/fac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(25, 32)</td>\n",
       "      <td>f</td>\n",
       "      <td>2395</td>\n",
       "      <td>876</td>\n",
       "      <td>771</td>\n",
       "      <td>771</td>\n",
       "      <td>AdienceBenchmarkGenderAndAgeClassification/fac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(25, 32)</td>\n",
       "      <td>m</td>\n",
       "      <td>752</td>\n",
       "      <td>1255</td>\n",
       "      <td>484</td>\n",
       "      <td>485</td>\n",
       "      <td>AdienceBenchmarkGenderAndAgeClassification/fac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(25, 32)</td>\n",
       "      <td>m</td>\n",
       "      <td>175</td>\n",
       "      <td>80</td>\n",
       "      <td>769</td>\n",
       "      <td>768</td>\n",
       "      <td>AdienceBenchmarkGenderAndAgeClassification/fac...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age gender     x     y    dx    dy  \\\n",
       "0  (25, 32)      f     0   414  1086  1383   \n",
       "1  (25, 32)      m   301   105   640   641   \n",
       "2  (25, 32)      f  2395   876   771   771   \n",
       "3  (25, 32)      m   752  1255   484   485   \n",
       "4  (25, 32)      m   175    80   769   768   \n",
       "\n",
       "                                            img_path  \n",
       "0  AdienceBenchmarkGenderAndAgeClassification/fac...  \n",
       "1  AdienceBenchmarkGenderAndAgeClassification/fac...  \n",
       "2  AdienceBenchmarkGenderAndAgeClassification/fac...  \n",
       "3  AdienceBenchmarkGenderAndAgeClassification/fac...  \n",
       "4  AdienceBenchmarkGenderAndAgeClassification/fac...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Defining data that we are going to use like age,gender,bounding box of all image\n",
    "imp_data = Data[['age', 'gender', 'x', 'y', 'dx', 'dy']].copy()\n",
    "imp_data.info()\n",
    "\n",
    "img_path = []\n",
    "for row in Data.iterrows():\n",
    "    path = \"AdienceBenchmarkGenderAndAgeClassification/faces/\"+row[1].user_id+\"/coarse_tilt_aligned_face.\"+str(row[1].face_id)+\".\"+row[1].original_image\n",
    "    img_path.append(path)\n",
    "\n",
    "imp_data['img_path'] = img_path\n",
    "imp_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78bb8c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mvsan\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1637: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25-32    5296\n",
       "38-43    2776\n",
       "0-2      2509\n",
       "8-13     2292\n",
       "4-6      2140\n",
       "15-20    1792\n",
       "48-53     916\n",
       "60+       901\n",
       "Name: age, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now we are mapping age into age_mapping which age group it is like that and drop the data that does not contain the value\n",
    "age_mapping = [('(0, 2)', '0-2'), ('2', '0-2'), ('3', '0-2'), ('(4, 6)', '4-6'), ('(8, 12)', '8-13'), ('13', '8-13'), ('22', '15-20'), ('(8, 23)','15-20'), ('23', '25-32'), ('(15, 20)', '15-20'), ('(25, 32)', '25-32'), ('(27, 32)', '25-32'), ('32', '25-32'), ('34', '25-32'), ('29', '25-32'), ('(38, 42)', '38-43'), ('35', '38-43'), ('36', '38-43'), ('42', '48-53'), ('45', '38-43'), ('(38, 43)', '38-43'), ('(38, 42)', '38-43'), ('(38, 48)', '48-53'), ('46', '48-53'), ('(48, 53)', '48-53'), ('55', '48-53'), ('56', '48-53'), ('(60, 100)', '60+'), ('57', '60+'), ('58', '60+')]\n",
    "age_mapping_dict = {each[0]: each[1] for each in age_mapping}\n",
    "drop_labels = []\n",
    "for idx, each in enumerate(imp_data.age):\n",
    "    if each == 'None':\n",
    "        drop_labels.append(idx)\n",
    "    else:\n",
    "        imp_data.age.loc[idx] = age_mapping_dict[each]\n",
    "\n",
    "imp_data = imp_data.drop(labels=drop_labels, axis=0) #droped None values\n",
    "imp_data.age.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "064eb116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 17452 entries, 0 to 19345\n",
      "Data columns (total 7 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   age       17452 non-null  object\n",
      " 1   gender    17452 non-null  object\n",
      " 2   x         17452 non-null  int64 \n",
      " 3   y         17452 non-null  int64 \n",
      " 4   dx        17452 non-null  int64 \n",
      " 5   dy        17452 non-null  int64 \n",
      " 6   img_path  17452 non-null  object\n",
      "dtypes: int64(4), object(3)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "#we have 'u' unknown data that we drop from the data so we can use the cleaned data to train our model\n",
    "imp_data = imp_data.dropna()\n",
    "clean_data = imp_data[imp_data.gender != 'u'].copy()\n",
    "clean_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f572917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>dx</th>\n",
       "      <th>dy</th>\n",
       "      <th>img_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25-32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>414</td>\n",
       "      <td>1086</td>\n",
       "      <td>1383</td>\n",
       "      <td>AdienceBenchmarkGenderAndAgeClassification/fac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25-32</td>\n",
       "      <td>1</td>\n",
       "      <td>301</td>\n",
       "      <td>105</td>\n",
       "      <td>640</td>\n",
       "      <td>641</td>\n",
       "      <td>AdienceBenchmarkGenderAndAgeClassification/fac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25-32</td>\n",
       "      <td>0</td>\n",
       "      <td>2395</td>\n",
       "      <td>876</td>\n",
       "      <td>771</td>\n",
       "      <td>771</td>\n",
       "      <td>AdienceBenchmarkGenderAndAgeClassification/fac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25-32</td>\n",
       "      <td>1</td>\n",
       "      <td>752</td>\n",
       "      <td>1255</td>\n",
       "      <td>484</td>\n",
       "      <td>485</td>\n",
       "      <td>AdienceBenchmarkGenderAndAgeClassification/fac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25-32</td>\n",
       "      <td>1</td>\n",
       "      <td>175</td>\n",
       "      <td>80</td>\n",
       "      <td>769</td>\n",
       "      <td>768</td>\n",
       "      <td>AdienceBenchmarkGenderAndAgeClassification/fac...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  gender     x     y    dx    dy  \\\n",
       "0  25-32       0     0   414  1086  1383   \n",
       "1  25-32       1   301   105   640   641   \n",
       "2  25-32       0  2395   876   771   771   \n",
       "3  25-32       1   752  1255   484   485   \n",
       "4  25-32       1   175    80   769   768   \n",
       "\n",
       "                                            img_path  \n",
       "0  AdienceBenchmarkGenderAndAgeClassification/fac...  \n",
       "1  AdienceBenchmarkGenderAndAgeClassification/fac...  \n",
       "2  AdienceBenchmarkGenderAndAgeClassification/fac...  \n",
       "3  AdienceBenchmarkGenderAndAgeClassification/fac...  \n",
       "4  AdienceBenchmarkGenderAndAgeClassification/fac...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We are mapping male as 1 and female as 0 for easy to access or easy way of doing in the code\n",
    "gender_to_label_map = {'f' : 0,'m' : 1}\n",
    "clean_data['gender'] = clean_data['gender'].apply(lambda g: gender_to_label_map[g])\n",
    "clean_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fed08f72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>dx</th>\n",
       "      <th>dy</th>\n",
       "      <th>img_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>414</td>\n",
       "      <td>1086</td>\n",
       "      <td>1383</td>\n",
       "      <td>AdienceBenchmarkGenderAndAgeClassification/fac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>301</td>\n",
       "      <td>105</td>\n",
       "      <td>640</td>\n",
       "      <td>641</td>\n",
       "      <td>AdienceBenchmarkGenderAndAgeClassification/fac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2395</td>\n",
       "      <td>876</td>\n",
       "      <td>771</td>\n",
       "      <td>771</td>\n",
       "      <td>AdienceBenchmarkGenderAndAgeClassification/fac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>752</td>\n",
       "      <td>1255</td>\n",
       "      <td>484</td>\n",
       "      <td>485</td>\n",
       "      <td>AdienceBenchmarkGenderAndAgeClassification/fac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>175</td>\n",
       "      <td>80</td>\n",
       "      <td>769</td>\n",
       "      <td>768</td>\n",
       "      <td>AdienceBenchmarkGenderAndAgeClassification/fac...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  gender     x     y    dx    dy  \\\n",
       "0    4       0     0   414  1086  1383   \n",
       "1    4       1   301   105   640   641   \n",
       "2    4       0  2395   876   771   771   \n",
       "3    4       1   752  1255   484   485   \n",
       "4    4       1   175    80   769   768   \n",
       "\n",
       "                                            img_path  \n",
       "0  AdienceBenchmarkGenderAndAgeClassification/fac...  \n",
       "1  AdienceBenchmarkGenderAndAgeClassification/fac...  \n",
       "2  AdienceBenchmarkGenderAndAgeClassification/fac...  \n",
       "3  AdienceBenchmarkGenderAndAgeClassification/fac...  \n",
       "4  AdienceBenchmarkGenderAndAgeClassification/fac...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mapping age from 0 to 7 so we will known the age of particular image taken.\n",
    "age_to_label_map = {'0-2'  :0,'4-6'  :1,'8-13' :2,'15-20':3,'25-32':4,'38-43':5,'48-53':6,'60+'  :7}\n",
    "clean_data['age'] = clean_data['age'].apply(lambda age: age_to_label_map[age])\n",
    "clean_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08ee5cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape (12216, 1)\n",
      "Test data shape (5236, 1)\n",
      "Train images shape (12216, 227, 227, 3)\n",
      "Test images shape (5236, 227, 227, 3)\n"
     ]
    }
   ],
   "source": [
    "#Now we are creating training and test dataset with sklearn train and test split method and os to read the file from its location\n",
    "import os\n",
    "X = clean_data[['img_path']]\n",
    "y = clean_data[['gender']]\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "print('Train data shape {}'.format(X_train.shape))\n",
    "print('Test data shape {}'.format(X_test.shape))\n",
    "\n",
    "train_images = []\n",
    "test_images = []\n",
    "\n",
    "for row in X_train.iterrows():\n",
    "    image = Image.open(row[1].img_path)\n",
    "    image = image.resize((227, 227))   # Resize the image\n",
    "    data = np.asarray(image)\n",
    "    train_images.append(data)\n",
    "\n",
    "for row in X_test.iterrows():\n",
    "    image = Image.open(row[1].img_path)\n",
    "    image = image.resize((227, 227))  # Resize the image\n",
    "    data = np.asarray(image)\n",
    "    test_images.append(data)\n",
    "\n",
    "train_images = np.asarray(train_images)\n",
    "test_images = np.asarray(test_images)\n",
    "\n",
    "print('Train images shape {}'.format(train_images.shape))\n",
    "print('Test images shape {}'.format(test_images.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9bf8d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 56, 56, 96)        14208     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 28, 28, 96)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " layer_normalization (LayerN  (None, 28, 28, 96)       192       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 28, 28, 256)       614656    \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 14, 14, 256)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " layer_normalization_1 (Laye  (None, 14, 14, 256)      512       \n",
      " rNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 14, 14, 256)       590080    \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 7, 7, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " layer_normalization_2 (Laye  (None, 7, 7, 256)        512       \n",
      " rNormalization)                                                 \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 12544)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               6423040   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 1026      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,906,882\n",
      "Trainable params: 7,906,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#In this we are creating model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(input_shape=(227, 227, 3), filters=96, kernel_size=(7, 7), strides=4, padding='valid', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
    "model.add(LayerNormalization())\n",
    "model.add(Conv2D(filters=256, kernel_size=(5, 5), strides=1, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
    "model.add(LayerNormalization())\n",
    "model.add(Conv2D(filters=256, kernel_size=(3, 3), strides=1, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
    "model.add(LayerNormalization())\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=512, activation='relu'))\n",
    "model.add(Dropout(rate=0.25))\n",
    "model.add(Dense(units=512, activation='relu'))\n",
    "model.add(Dropout(rate=0.25))\n",
    "model.add(Dense(units=2, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd4a5373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mvsan\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382/382 [==============================] - ETA: 0s - loss: 0.9861 - accuracy: 0.5288"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mvsan\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382/382 [==============================] - 621s 2s/step - loss: 0.9861 - accuracy: 0.5288 - val_loss: 0.6926 - val_accuracy: 0.5241\n",
      "Epoch 2/25\n",
      "382/382 [==============================] - 1135s 3s/step - loss: 0.6902 - accuracy: 0.5393 - val_loss: 0.6926 - val_accuracy: 0.5241\n",
      "Epoch 3/25\n",
      "382/382 [==============================] - 636s 2s/step - loss: 0.6902 - accuracy: 0.5393 - val_loss: 0.6927 - val_accuracy: 0.5241\n",
      "Epoch 4/25\n",
      "382/382 [==============================] - 956s 3s/step - loss: 0.6903 - accuracy: 0.5393 - val_loss: 0.6921 - val_accuracy: 0.5241\n",
      "Epoch 5/25\n",
      "382/382 [==============================] - 639s 2s/step - loss: 0.6902 - accuracy: 0.5393 - val_loss: 0.6921 - val_accuracy: 0.5241\n",
      "Epoch 6/25\n",
      "382/382 [==============================] - 632s 2s/step - loss: 0.6902 - accuracy: 0.5393 - val_loss: 0.6922 - val_accuracy: 0.5241\n",
      "Epoch 7/25\n",
      "382/382 [==============================] - 697s 2s/step - loss: 0.6901 - accuracy: 0.5393 - val_loss: 0.6924 - val_accuracy: 0.5241\n",
      "Epoch 8/25\n",
      "382/382 [==============================] - 681s 2s/step - loss: 0.6902 - accuracy: 0.5393 - val_loss: 0.6921 - val_accuracy: 0.5241\n",
      "Epoch 9/25\n",
      "382/382 [==============================] - 679s 2s/step - loss: 0.6901 - accuracy: 0.5393 - val_loss: 0.6930 - val_accuracy: 0.5241\n",
      "Epoch 10/25\n",
      "382/382 [==============================] - 671s 2s/step - loss: 0.6902 - accuracy: 0.5393 - val_loss: 0.6923 - val_accuracy: 0.5241\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "#we train the model and saving the model as well\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3) # Callback for earlystopping\n",
    "model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "history = model.fit(train_images, y_train, batch_size=32, epochs=25, validation_data=(test_images, y_test), callbacks=[callback])\n",
    "\n",
    "print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "\n",
    "model.save('gender_model25.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4871fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape (12216, 1)\n",
      "Test data shape (5236, 1)\n",
      "Train images shape (12216, 227, 227, 3)\n",
      "Test images shape (5236, 227, 227, 3)\n"
     ]
    }
   ],
   "source": [
    "#Now we create training and testing split for age data\n",
    "X = clean_data[['img_path']]\n",
    "y = clean_data[['age']]\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "print('Train data shape {}'.format(X_train.shape))\n",
    "print('Test data shape {}'.format(X_test.shape))\n",
    "\n",
    "train_images = []\n",
    "test_images = []\n",
    "\n",
    "for row in X_train.iterrows():\n",
    "    image = Image.open(row[1].img_path)\n",
    "    image = image.resize((227, 227))   # Resize the image\n",
    "    data = np.asarray(image)\n",
    "    train_images.append(data)\n",
    "\n",
    "for row in X_test.iterrows():\n",
    "    image = Image.open(row[1].img_path)\n",
    "    image = image.resize((227, 227))  # Resize the image\n",
    "    data = np.asarray(image)\n",
    "    test_images.append(data)\n",
    "\n",
    "train_images = np.asarray(train_images)\n",
    "test_images = np.asarray(test_images)\n",
    "\n",
    "print('Train images shape {}'.format(train_images.shape))\n",
    "print('Test images shape {}'.format(test_images.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42d5c181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 56, 56, 96)        14208     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 28, 28, 96)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " layer_normalization_3 (Laye  (None, 28, 28, 96)       192       \n",
      " rNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 28, 28, 256)       614656    \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 14, 14, 256)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " layer_normalization_4 (Laye  (None, 14, 14, 256)      512       \n",
      " rNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 14, 14, 256)       590080    \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 7, 7, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " layer_normalization_5 (Laye  (None, 7, 7, 256)        512       \n",
      " rNormalization)                                                 \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 12544)             0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 512)               6423040   \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 8)                 4104      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,909,960\n",
      "Trainable params: 7,909,960\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Building age model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(input_shape=(227, 227, 3), filters=96, kernel_size=(7, 7), strides=4, padding='valid', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
    "model.add(LayerNormalization())\n",
    "model.add(Conv2D(filters=256, kernel_size=(5, 5), strides=1, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
    "model.add(LayerNormalization())\n",
    "model.add(Conv2D(filters=256, kernel_size=(3, 3), strides=1, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
    "model.add(LayerNormalization())\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=512, activation='relu'))\n",
    "model.add(Dropout(rate=0.25))\n",
    "model.add(Dense(units=512, activation='relu'))\n",
    "model.add(Dropout(rate=0.25))\n",
    "model.add(Dense(units=8, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82a29de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mvsan\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382/382 [==============================] - ETA: 0s - loss: 2.2154 - accuracy: 0.2858"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mvsan\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382/382 [==============================] - 1820s 5s/step - loss: 2.2154 - accuracy: 0.2858 - val_loss: 1.9210 - val_accuracy: 0.2976\n",
      "Epoch 2/50\n",
      "382/382 [==============================] - 645s 2s/step - loss: 1.9228 - accuracy: 0.3014 - val_loss: 1.9209 - val_accuracy: 0.2976\n",
      "Epoch 3/50\n",
      "382/382 [==============================] - 628s 2s/step - loss: 1.9235 - accuracy: 0.3014 - val_loss: 1.9211 - val_accuracy: 0.2976\n",
      "Epoch 4/50\n",
      "382/382 [==============================] - 627s 2s/step - loss: 1.9225 - accuracy: 0.3014 - val_loss: 1.9212 - val_accuracy: 0.2976\n",
      "Epoch 5/50\n",
      "382/382 [==============================] - 629s 2s/step - loss: 1.9241 - accuracy: 0.3014 - val_loss: 1.9206 - val_accuracy: 0.2976\n",
      "Epoch 6/50\n",
      "382/382 [==============================] - 631s 2s/step - loss: 1.9236 - accuracy: 0.3014 - val_loss: 1.9216 - val_accuracy: 0.2976\n",
      "Epoch 7/50\n",
      "382/382 [==============================] - 629s 2s/step - loss: 1.9235 - accuracy: 0.3014 - val_loss: 1.9206 - val_accuracy: 0.2976\n",
      "164/164 - 69s - loss: 1.9206 - accuracy: 0.2976 - 69s/epoch - 420ms/step\n",
      "0.2975553870201111\n"
     ]
    }
   ],
   "source": [
    "#we train the model and saving the model as well\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3) # Callback for earlystopping\n",
    "model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "history = model.fit(train_images, y_train, batch_size=32, epochs=50, validation_data=(test_images, y_test), callbacks=[callback])\n",
    "\n",
    "model.save('age_model50.h5')\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_images, y_test, verbose=2)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c2ce58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e0ef65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72997155",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
